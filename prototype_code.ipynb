{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fb1035-1589-4e7b-9962-aecea8d37860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b42236-d6ea-49db-829f-d98d01de43e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 14:09:53 WARN Utils: Your hostname, Gunnvants-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.184 instead (on interface en0)\n",
      "22/12/28 14:09:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 14:09:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('sparkify_etl').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca84e1b-6a73-4507-aa7b-2bfe7a1e80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_songs = '/Users/gunnvantsaini/Library/CloudStorage/OneDrive-Personal/project_codes/backend/data_engineering/cloud_dw/scripts/song_data/*/*/*/*.json'\n",
    "source_logs = '/Users/gunnvantsaini/Library/CloudStorage/OneDrive-Personal/project_codes/backend/data_engineering/cloud_dw/scripts/log_data/*/*/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16496e18-9059-4177-b6b2-67bb7ada2625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs = spark.read.json(source_songs)\n",
    "logs = spark.read.json(source_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74921c9a-57cf-49e3-941b-1e7f93bf3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14896"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c19cfe-92d3-46d6-81c5-9fcff8c734a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df2db3ca-5faa-413d-9601-db92005a2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d527951a-df58-471b-8db6-d6e06b2ac9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114b86d6-259c-480c-9e01-54acaf8f09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_cols = ['userId', 'firstName', 'lastName', 'gender', 'level']\n",
    "song_table_cols = ['song_id', 'title', 'artist_id', 'year', 'duration']\n",
    "artist_table_cols = ['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8faecda0-a6a8-4b3a-b02d-03417ec793a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table=logs.select(user_table_cols).drop_duplicates()\n",
    "songs_table=songs.select(song_table_cols).drop_duplicates()\n",
    "artist_table=songs.select(artist_table_cols).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3289b4cd-5360-4538-a112-34131385b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf as udf\n",
    "from pyspark.sql import types as t\n",
    "@udf(t.FloatType())\n",
    "def norm_ts(v):\n",
    "    return v/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "195eae59-5dab-4ed9-96f2-b3d535e1ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.withColumn(\"start_time\",f.from_unixtime(norm_ts(\"ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40a148f4-5802-44fe-b479-5b1856b5b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = logs.\\\n",
    "            withColumn('hour',f.hour('start_time')).\\\n",
    "            withColumn(\"day\",f.dayofmonth(\"start_time\")).\\\n",
    "            withColumn(\"week\",f.dayofweek(\"start_time\")).\\\n",
    "            withColumn(\"month\",f.month(\"start_time\")).\\\n",
    "            withColumn(\"year\",f.month(\"start_time\")).distinct()\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cffc9d9c-5116-4ebd-8ba1-6048f20bfd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['start_time', 'userId', 'level', 'song_id', 'artist_id', 'sessionId', 'location', 'userAgent']\n",
    "song_plays = songs.\\\n",
    "        join(logs.where(\"page=='NextSong'\"),songs.artist_name==logs.artist,'inner').\\\n",
    "        select(cols).distinct().\\\n",
    "        withColumn(\"songplay_id\",f.monotonically_increasing_id()).\\\n",
    "        withColumn(\"year\",f.year(\"start_time\")).\\\n",
    "        withColumn(\"month\",f.month(\"start_time\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54124469-21f7-48ed-a0f6-c7e20364e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs_table.write.partitionBy(['year','artist_id']).parquet(\"songs_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76e617ed-e343-44c8-b49a-774e2499bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "artist_table.write.parquet(\"artist_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425806fb-e994-4a41-bd55-5b6dc3aff6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table.write.parquet(\"user_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09fe3ada-1865-48fd-8818-b0f558d771ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "time_table.write.partitionBy('year','month').parquet(\"time_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50938aab-ecc9-4da3-a1d1-eb982668f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 15:25:28 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/7h/01sjg2sx6tl3y72r2klm24c80000gn/T/blockmgr-3b887eea-2b37-42de-962b-c31926e161fd. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/7h/01sjg2sx6tl3y72r2klm24c80000gn/T/blockmgr-3b887eea-2b37-42de-962b-c31926e161fd\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:365)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2026)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 58076)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gunnvantsaini/miniforge3/envs/spark/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/gunnvantsaini/miniforge3/envs/spark/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/gunnvantsaini/miniforge3/envs/spark/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/gunnvantsaini/miniforge3/envs/spark/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/gunnvantsaini/spark-3.3.1-bin-hadoop3/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/gunnvantsaini/spark-3.3.1-bin-hadoop3/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/Users/gunnvantsaini/spark-3.3.1-bin-hadoop3/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/gunnvantsaini/spark-3.3.1-bin-hadoop3/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "song_plays.write.partitionBy('year','month').parquet('songplays_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081503be-f691-4659-9e08-965557aa9f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
